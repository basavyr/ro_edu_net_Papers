______
The original introduction
_____




Started as a protocol which granted remote access to root format specific files, with a primary use case focused on data analysis (rather than data transfer), XRootD became widely used within the scientific community at CERN (European Organization for Nuclear Research) and other large-scale facilities (e.g., SLAC-Stanford Linear Accelerator Center). Over the last years, the framework evolved a lot, and it now supports data analysis, data transfers, data management, and also features like staging data from tape. 

In terms of storage capacity, only the ATLAS and CMS collaborations alone produce a total of around 150 Petabytes of data which needs to be accessed by thousands of physicists within the Worldwide Large Hadron Collider Compute Grid - WLCG community [7]. As a result, a key objective of the WLCG is to assure both the process of moving the data between sites and deliver the data to any end-user application.

The addition of XRootD on top of the Anydata, Anytime, Anywhere project (or AAA for short) [2] allowed the high Energy Physics community to achieve global data storage federations that have a single data-access entry point and also a common data-access protocol, which changed the old paradigm of distributed multi-tiered storage. This is possible through the hierarchical deployment of redirectors that allow real-time discovery of sites that are hosting the data. The XRootD ability to federate different sides through meta managers together with additional functionalities provided by the AAA (like logical file name translation to physical file name) allowed to achieve a global, multi-site environment for data storage and analysis.

To emphasize the importance of XRootD, it is worth mentioning that currently at CERN, the main storage solution is EOS â€“ a technology developed in-house and built on top of the XRootD framework. The LHC experiments (e.g. ATLAS, CMS, LHCb, ALICE) and also other smaller experiments hosted at CERN (e.g. AMS), including their user communities, use EOS for data storage and access. In the following section, we provide a clearer picture of the XRootD framework, both in terms of its server-side as well as its client-side, since both implementations are crucial in understanding the overall workflow of data access and data manipulation within the WLCG community.

____
Revised introduction
____

In terms of its functionality, the XRootD framework is composed of a server-side and a client-side. Each component will be described in detail in the following sections, however it is worth mentioning that the interaction of any end-user with the XRootD framework (in the process of accessing stored data) will be through the client interface (or shortly XrdCl). Making sure that the stored data from any of the facilities which run experiments is available to the user, and assuring a constant transfer bandwidth even when the distributed storage system is accessed by a multitude of clients represents a real challenge, especially when considering that, as in any kind of data storage facility, equipment may fail (for example disks that stop working could lead to entire storage blocks to be unresponsive). Nowadays, with the larger capacity disk drivers, when one disk fails it can take days to rebuild data and restore processes. Not having access to the desired data or even losing it completely proves to be the worst case scenario for the entire community (for both the service providers and the end-user). As a result, in the case of large data centers, a crucial aspect is the efficient repair of failed (storage) nodes and the reliability of the data. Both can be ensured by introducing redundancy (e.g., ranging from straightforward replication to the use of complex schemes that minimize the storage overhead while maximizing the reliability). A very efficient method that aims at such a thing is the so-called Erasure Coding (EC) scheme. It is important to understand that EC started to gain significant interest in the design of storage systems that offer low (minimal) overhead and minimize the repair times of faulty storage nodes.

Implementing an Erasure Coding mechanism within the XRootD framework would represent a great accomplishment for the scientific community at CERN. The present work aims at showing the development steps of such a tool, by describing how a plug-in mechanism that improves the data redundancy and reliability is created. Moreover, the workflow is chosen in such a way that code efficiency and optimization are the priorities - using a so-called Declarative API (a new feature of the XRootD client). The adopted approach will be compared with an existing API, and conclusion that the former method is more efficient will be drawn.
